{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35083489",
   "metadata": {},
   "source": [
    "# Ensemble Models\n",
    "Ensemble methods are powerful machine learning techniques that combine predictions from multiple models to improve overall performance. Instead of relying on a single model, ensemble methods leverage the collective intelligence of several models, often leading to better accuracy, robustness, and generalization. Ensemble methods operate under the principle that a group of models (an ensemble) can outperform a single model when their individual strengths and weaknesses are combined effectively. These methods are particularly useful for reducing variance, bias, or improving prediction accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8996e3a2",
   "metadata": {},
   "source": [
    "# Bagging\n",
    "Bagging (Bootstrap Aggregating) is an ensemble method that reduces variance by training multiple versions of the same model on different random subsets of the training data, sampled with replacement. Each model is trained independently, and their predictions are combined—through averaging for regression or majority voting for classification—to produce a more accurate and robust result. Bagging works best with high-variance models like decision trees, as it mitigates overfitting while retaining predictive power. Random Forest is a popular example of bagging, adding further randomness by selecting a subset of features at each tree split.\n",
    "\n",
    "# Boosting\n",
    "\n",
    "Boosting is an ensemble method that reduces bias by training models sequentially, where each new model focuses on correcting the errors made by its predecessor. Weak learners, often simple models like shallow decision trees, are combined iteratively to form a strong model. Boosting assigns higher importance to misclassified or poorly predicted instances, improving overall accuracy. Unlike bagging, boosting models are dependent on each other, making the process more sensitive to noise but highly effective for complex problems. Popular boosting algorithms include AdaBoost and XGBoost, known for their ability to achieve high predictive performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15628f36",
   "metadata": {},
   "source": [
    "# Adaboost "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8446e5bc",
   "metadata": {},
   "source": [
    "Adaboost, or Adaptive Boosting, is an ensemble learning algorithm that combines multiple weak classifiers to form a single strong classifier. The algorithm focuses on difficult-to-classify samples by iteratively adjusting the weights of training data points. This adaptive process ensures that the model progressively improves its performance on challenging examples. Adaboost is primarily used for binary classification problems but can also be extended to handle multi-class scenarios.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692091cf",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f9a9e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550e41d7",
   "metadata": {},
   "source": [
    "### Implementing Adaboost\n",
    "During initialization, it sets the number of weak classifiers, and stores their weights and models. The train method iteratively trains weak classifiers (decision stumps) by calculating weighted errors and adjusting sample weights to focus on misclassified samples. Each stump is assigned a weight based on its accuracy. The _train_stump method identifies the best stump by evaluating all features, thresholds, and polarities, minimizing weighted classification error. The predict method combines the predictions of all stumps using their weights, yielding the final ensemble prediction by taking the sign of the weighted sum. This method efficiently boosts weak learners into a strong classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af0efe57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adaboost:\n",
    "    def __init__(self, n_estimators=50):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.alphas = []\n",
    "        self.models = []\n",
    "\n",
    "    def train(self, X, y):\n",
    "        n_samples, _ = X.shape\n",
    "        weights = np.ones(n_samples) / n_samples\n",
    "\n",
    "        for _ in range(self.n_estimators):\n",
    "            stump = self._train_stump(X, y, weights)\n",
    "            predictions = stump['predictions']\n",
    "            error = np.sum(weights * (predictions != y))\n",
    "\n",
    "            alpha = 0.5 * np.log((1 - error) / (error + 1e-10))\n",
    "            self.alphas.append(alpha)\n",
    "            self.models.append(stump)\n",
    "\n",
    "            weights *= np.exp(-alpha * y * predictions)\n",
    "            weights /= np.sum(weights)\n",
    "\n",
    "    def _train_stump(self, X, y, weights):\n",
    "        n_samples, n_features = X.shape\n",
    "        min_error = float('inf')\n",
    "        stump = {}\n",
    "\n",
    "        for feature_idx in range(n_features):\n",
    "            thresholds = np.unique(X[:, feature_idx])\n",
    "            for threshold in thresholds:\n",
    "                for polarity in [1, -1]:\n",
    "                    predictions = np.ones(n_samples)\n",
    "                    predictions[polarity * X[:, feature_idx] < polarity * threshold] = -1\n",
    "                    error = np.sum(weights * (predictions != y))\n",
    "\n",
    "                    if error < min_error:\n",
    "                        min_error = error\n",
    "                        stump = {\n",
    "                            'feature_idx': feature_idx,\n",
    "                            'threshold': threshold,\n",
    "                            'polarity': polarity,\n",
    "                            'predictions': predictions\n",
    "                        }\n",
    "        return stump\n",
    "\n",
    "    def predict(self, X):\n",
    "        n_samples = X.shape[0]\n",
    "        final_predictions = np.zeros(n_samples)\n",
    "\n",
    "        for alpha, model in zip(self.alphas, self.models):\n",
    "            predictions = np.ones(n_samples)\n",
    "            feature_idx = model['feature_idx']\n",
    "            threshold = model['threshold']\n",
    "            polarity = model['polarity']\n",
    "            predictions[polarity * X[:, feature_idx] < polarity * threshold] = -1\n",
    "            final_predictions += alpha * predictions\n",
    "\n",
    "        return np.sign(final_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4308ea",
   "metadata": {},
   "source": [
    "### Loading Data\n",
    "\n",
    "We generate a synthetic binary classification dataset using make_classification, creating 500 samples with 5 features and 2 classes, while ensuring reproducibility with a fixed random seed. The class labels are initially 0 and 1, but they are converted to -1 and 1 using np.where to align with algorithms like Adaboost that require binary labels in this format. The dataset is then split into training and testing sets, with the first 350 samples used for training and the remaining 150 for testing. This setup provides a well-structured dataset for evaluating classification models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df3bf86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=500, n_features=5, n_classes=2, random_state=42)\n",
    "y = np.where(y == 0, -1, 1)\n",
    "\n",
    "train_size = 350\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd3e652",
   "metadata": {},
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "246f01d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "adaboost = Adaboost(n_estimators=50)\n",
    "adaboost.train(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebfc4c4",
   "metadata": {},
   "source": [
    "### Prediction and Testing\n",
    "We evaluates the performance of the Adaboost model on the test dataset. It begins by predicting the class labels for the test data (X_test) using the model's predict method, storing the results in y_pred. The confusion_matrix function computes the number of true positives, true negatives, false positives, and false negatives, returning a 2x2 matrix summarizing the classification results. The accuracy_score function calculates the fraction of correctly predicted samples by comparing y_true and y_pred. The precision_score function calculates precision, defined as the ratio of true positives to all predicted positives, ensuring no division by zero. The confusion matrix, accuracy, and precision are then printed to provide a detailed summary of the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "878b7f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[65  4]\n",
      " [10 71]]\n",
      "\n",
      "Accuracy: 0.9066666666666666\n",
      "Precision: 0.9420289855072463\n"
     ]
    }
   ],
   "source": [
    "y_pred = adaboost.predict(X_test)\n",
    "\n",
    "def confusion_matrix(y_true, y_pred):\n",
    "    TP = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    TN = np.sum((y_true == -1) & (y_pred == -1))\n",
    "    FP = np.sum((y_true == -1) & (y_pred == 1))\n",
    "    FN = np.sum((y_true == 1) & (y_pred == -1))\n",
    "    return np.array([[TP, FP], [FN, TN]])\n",
    "\n",
    "def accuracy_score(y_true, y_pred):\n",
    "    return np.sum(y_true == y_pred) / len(y_true)\n",
    "\n",
    "def precision_score(y_true, y_pred):\n",
    "    TP = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    FP = np.sum((y_true == -1) & (y_pred == 1))\n",
    "    return TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "\n",
    "print(\"Confusion Matrix:\\n\", cm, end=\"\\n\\n\")\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b8d51a",
   "metadata": {},
   "source": [
    "## Theory for Adaboost Algorithm\n",
    "\n",
    "The Adaboost algorithm begins by initializing the weights of all training samples equally. For a dataset with $N$ samples, the initial weight for each sample is given by:\n",
    "\n",
    "$$\n",
    "w_i = \\frac{1}{N}, \\quad \\forall i \\in \\{1, 2, \\dots, N\\}\n",
    "$$\n",
    "\n",
    "A weak classifier $h_m(x)$ is then trained at iteration $m$. This classifier minimizes the weighted classification error, calculated as:\n",
    "\n",
    "$$\n",
    "\\epsilon_m = \\frac{\\sum_{i=1}^{N} w_i \\cdot \\mathbb{I}(y_i \\neq h_m(x_i))}{\\sum_{i=1}^{N} w_i}\n",
    "$$\n",
    "\n",
    "Here, $\\mathbb{I}$ is the indicator function, which is 1 if the condition is true and 0 otherwise.\n",
    "\n",
    "The weight of the weak classifier, $\\alpha_m$, is then computed as:\n",
    "\n",
    "$$\n",
    "\\alpha_m = \\frac{1}{2} \\ln\\left(\\frac{1 - \\epsilon_m}{\\epsilon_m}\\right)\n",
    "$$\n",
    "\n",
    "The weights of the training samples are updated to emphasize misclassified examples. The update rule is:\n",
    "\n",
    "$$\n",
    "w_i \\leftarrow w_i \\cdot \\exp\\left(-\\alpha_m \\cdot y_i \\cdot h_m(x_i)\\right)\n",
    "$$\n",
    "\n",
    "The weights are then normalized so that they sum to 1:\n",
    "\n",
    "$$\n",
    "w_i \\leftarrow \\frac{w_i}{\\sum_{i=1}^{N} w_i}\n",
    "$$\n",
    "\n",
    "Finally, after all iterations, the strong classifier is constructed as a weighted combination of the weak classifiers:\n",
    "\n",
    "$$\n",
    "H(x) = \\text{sign}\\left(\\sum_{m=1}^{M} \\alpha_m \\cdot h_m(x)\\right)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e84ca4",
   "metadata": {},
   "source": [
    "# XGBoost\n",
    "XGBoost (eXtreme Gradient Boosting) is a highly efficient and scalable implementation of gradient-boosted decision trees. It is widely used for supervised learning tasks, particularly with structured or tabular data, due to its performance and robustness. XGBoost stands out because of its ability to handle missing data, its use of regularization techniques to prevent overfitting, and its speed, thanks to its parallelized tree-building process.\n",
    "\n",
    "In this notebook, we will implement XGBoost using only NumPy and Pandas libraries, focusing on manually defining the steps involved in the algorithm. This approach provides a deeper understanding of how XGBoost works internally.\n",
    "\n",
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3160fe12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78436f17",
   "metadata": {},
   "source": [
    "### Dataset Description\n",
    "\n",
    "The dataset we use in this notebook contains features and a target variable for a classification task. Each row represents an observation, with columns for input features and the corresponding output label. For demonstration purposes, we assume the dataset has been preprocessed to handle missing values and categorical data. The focus here will be on implementing XGBoost rather than data cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b17ba21b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Feature_1  Feature_2  Target\n",
      "0   2.496714   1.861736     0.0\n",
      "1   2.647689   3.523030     0.0\n",
      "2   1.765847   1.765863     0.0\n",
      "3   3.579213   2.767435     0.0\n",
      "4   1.530526   2.542560     0.0\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"testyyy.csv\")\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32fd0d2",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "Before training our model, we need to split the dataset into features and labels, followed by dividing the data into training and test sets. Features refer to the input variables used for making predictions, while the labels are the target variables. We will use 80% of the data for training and 20% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "afeacc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:, :-1].values  \n",
    "y = data.iloc[:, -1].values\n",
    "\n",
    "train_size = int(0.8 * len(X))\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19352a6a",
   "metadata": {},
   "source": [
    "### XGBoost Implementation\n",
    "\n",
    "XGBoost uses gradient boosting to optimize an objective function. It builds trees sequentially, where each tree is trained to correct the residual errors of the previous trees. Initialize predictions with a constant value (e.g., the mean of the target). Iteratively fit decision trees to the negative gradient of the loss function. Update predictions by adding the weighted outputs of the trees. Use a learning rate to scale the updates, controlling the contribution of each tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74905e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = 100  \n",
    "learning_rate = 0.1  \n",
    "max_depth = 3  \n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "predictions = np.full_like(y_train, np.mean(y_train), dtype=np.float64)\n",
    "\n",
    "for estimator in range(n_estimators):\n",
    "    residuals = y_train - predictions\n",
    "    tree_output = np.sign(residuals) \n",
    "    predictions += learning_rate * tree_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae725ad0",
   "metadata": {},
   "source": [
    "### Predictions and Evaluation\n",
    "\n",
    "After training, we use the final model to predict on the test set. Predictions are made by summing the contributions of all trees and applying the sigmoid function to convert them into probabilities. These probabilities are thresholded at 0.5 to generate binary predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d65dab1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "train_predictions = np.full_like(y_train, np.mean(y_train), dtype=np.float64)\n",
    "test_predictions = np.full_like(y_test, np.mean(y_train), dtype=np.float64)\n",
    "\n",
    "def simple_tree(X, residuals):\n",
    "    n_features = X.shape[1]\n",
    "    best_split = None\n",
    "    min_error = float(\"inf\")\n",
    "    \n",
    "    for feature in range(n_features):\n",
    "        threshold = np.median(X[:, feature])\n",
    "        left_idx = X[:, feature] <= threshold\n",
    "        right_idx = X[:, feature] > threshold\n",
    "        \n",
    "        left_residuals = residuals[left_idx]\n",
    "        right_residuals = residuals[right_idx]\n",
    "        \n",
    "        error = (np.mean(left_residuals) ** 2) * len(left_residuals) + \\\n",
    "                (np.mean(right_residuals) ** 2) * len(right_residuals)\n",
    "        \n",
    "        if error < min_error:\n",
    "            min_error = error\n",
    "            best_split = (feature, threshold, np.mean(left_residuals), np.mean(right_residuals))\n",
    "    \n",
    "    return best_split\n",
    "\n",
    "for estimator in range(n_estimators):\n",
    "    residuals = y_train - train_predictions\n",
    "    \n",
    "    split = simple_tree(X_train, residuals)\n",
    "    if split is None:\n",
    "        break\n",
    "    \n",
    "    feature, threshold, left_val, right_val = split\n",
    "    \n",
    "    left_idx = X_train[:, feature] <= threshold\n",
    "    right_idx = X_train[:, feature] > threshold\n",
    "    train_predictions[left_idx] += learning_rate * left_val\n",
    "    train_predictions[right_idx] += learning_rate * right_val\n",
    "    \n",
    "    left_idx_test = X_test[:, feature] <= threshold\n",
    "    right_idx_test = X_test[:, feature] > threshold\n",
    "    test_predictions[left_idx_test] += learning_rate * left_val\n",
    "    test_predictions[right_idx_test] += learning_rate * right_val\n",
    "\n",
    "test_predictions = sigmoid(test_predictions)\n",
    "test_predictions_binary = (test_predictions > 0.5).astype(int)\n",
    "\n",
    "accuracy = np.mean(test_predictions_binary == y_test)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37f7b5c",
   "metadata": {},
   "source": [
    "## Theory for XGBoost\n",
    "\n",
    "XGBoost optimizes the following loss function:\n",
    "$$L(\\theta) = \\sum_{i=1}^n l(y_i, \\hat{y}_i) + \\sum_{k=1}^K \\Omega(f_k)$$\n",
    "\n",
    "Where:\n",
    "- $l$ is the loss function \n",
    "- $\\Omega(f_k)$ is the regularization term to penalize complexity\n",
    "- $f_k$ represents the $k$-th tree in the ensemble\n",
    "\n",
    "Each tree corrects the residuals of the previous ensemble, updating the predictions as follows:\n",
    "$$g_m(x) = g_{m-1}(x) + \\alpha h_m(x)$$\n",
    "\n",
    "Here, $\\alpha$ is the learning rate, and $h_m(x)$ is the output of the $m$-th tree. The process continues until the specified number of trees is trained or the residuals are sufficiently minimized."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
