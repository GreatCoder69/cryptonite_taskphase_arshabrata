{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K Nearest Neighbour\n",
    "## Introduction\n",
    "K-Nearest Neighbors (KNN) is a simple, yet powerful, supervised machine learning algorithm used for classification and regression tasks. The core idea behind KNN is to predict the class (or value) of a data point based on the majority class (or average value) of its 'k' nearest neighbors in the feature space. The algorithm works by calculating the distance between the query point and all other points in the dataset, typically using Euclidean distance. The nearest 'k' points are selected, and their class labels or values are used to make predictions. KNN is intuitive and does not require explicit training, but its performance can be influenced by factors such as the choice of distance metric, the value of 'k', and the dimensionality of the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation of Algorithm in Python\n",
    "### Import Required Libraries\n",
    "The essential libraries are imported in this step: numpy for numerical computations, pandas for handling and processing tabular datasets, random for shuffling the data, and Counter for tallying votes in the KNN algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize the Dataset\n",
    "This function applies Min-Max normalization to the feature columns of the dataset. The idea is to scale the features so that they lie within a fixed range, typically [0, 1]. This ensures that all features contribute equally to the KNN algorithmâ€™s distance computation. \n",
    "The function first extracts all feature columns (excluding the class label).\n",
    "It then computes the minimum and maximum values for each feature across the dataset.\n",
    "Each feature is normalized by subtracting the minimum value and dividing by the feature range (max - min).\n",
    "The class label is left unchanged, and the normalized feature values are appended with the original class label for each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_dataset(dataset):\n",
    "    normalized_data = []\n",
    "    features = np.array(dataset)[:, :-1]  \n",
    "    min_vals = np.min(features, axis=0)  \n",
    "    max_vals = np.max(features, axis=0)  \n",
    "    ranges = max_vals - min_vals\n",
    "    for row in dataset:\n",
    "        normalized_features = (row[:-1] - min_vals) / ranges\n",
    "        normalized_data.append(np.append(normalized_features, row[-1])) \n",
    "    return normalized_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Outliers\n",
    "This function removes outliers using the Z-score method. Outliers are data points that deviate significantly from other points and can negatively impact the KNN algorithm. The function calculates the mean and standard deviation for each feature. \n",
    "For each row, it computes the Z-score for each feature, which measures how many standard deviations a value is away from the mean. \n",
    "If any Z-score is greater than 3 or less than -3 (meaning the value is more than 3 standard deviations away from the mean), the row is considered an outlier and removed. \n",
    "Only rows where all features have Z-scores within the [-3, 3] range are retained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(dataset):\n",
    "    features = np.array(dataset)[:, :-1] \n",
    "    mean = np.mean(features, axis=0)  \n",
    "    std_dev = np.std(features, axis=0)  \n",
    "    cleaned_data = []\n",
    "    for row in dataset:\n",
    "        z_scores = (row[:-1] - mean) / std_dev  \n",
    "        if all(np.abs(z_scores) < 3): \n",
    "            cleaned_data.append(row)\n",
    "    return cleaned_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted K-Nearest Neighbors Algorithm\n",
    "This is the core implementation of the K-Nearest Neighbors (KNN) algorithm with weighted voting. In regular KNN, each of the k neighbors contributes equally to the classification. In this weighted version, closer neighbors have a greater influence on the classification decision. \n",
    "First, the Euclidean distance between the test point (predict) and all the points in the training data is computed.\n",
    "The k nearest neighbors are selected based on their distance.\n",
    "Each neighbor's vote is weighted inversely by the distance. The closer a neighbor is, the larger its weight.\n",
    "The class with the highest total weight from the neighbors is predicted.\n",
    "The confidence is calculated as the proportion of the total weight attributed to the predicted class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_nearest_neighbors(data, predict, k=3):   \n",
    "    distances = []\n",
    "    for group in data:\n",
    "        for features in data[group]:\n",
    "            euclidean_distance = np.linalg.norm(np.array(features) - np.array(predict))\n",
    "            distances.append([euclidean_distance, group])\n",
    "    distances = sorted(distances)[:k]\n",
    "    weights = [1 / d[0] if d[0] != 0 else 1 for d in distances]  # Inverse distance weighting\n",
    "    weighted_votes = {}\n",
    "    for i, (_, group) in enumerate(distances):\n",
    "        weighted_votes[group] = weighted_votes.get(group, 0) + weights[i]\n",
    "    votes_result = max(weighted_votes, key=weighted_votes.get)\n",
    "    confidence = (weighted_votes[votes_result] / sum(weights)) * 100\n",
    "    return votes_result, confidence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation\n",
    "Cross-validation is a technique used to assess the performance of the model more reliably. Instead of splitting the dataset once into a training set and a test set, the dataset is split into k folds. The model is trained on k-1 folds and tested on the remaining fold, and this process is repeated for all folds. \n",
    "The dataset is divided into k equal parts (folds).\n",
    "For each fold, the model is trained on the remaining k-1 folds and evaluated on the fold that was held out.\n",
    "The accuracy of the model is computed for each fold and then averaged to provide an overall performance measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate(dataset, k, folds=5):\n",
    "    fold_size = len(dataset) // folds\n",
    "    accuracies = []\n",
    "    for i in range(folds):\n",
    "        train_data = dataset[:i * fold_size] + dataset[(i + 1) * fold_size:]\n",
    "        test_data = dataset[i * fold_size:(i + 1) * fold_size]\n",
    "        train_set = {cls: [] for cls in set(row[-1] for row in dataset)}\n",
    "        test_set = {cls: [] for cls in set(row[-1] for row in dataset)}\n",
    "        for row in train_data:\n",
    "            train_set[row[-1]].append(row[:-1])\n",
    "        for row in test_data:\n",
    "            test_set[row[-1]].append(row[:-1])\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for group in test_set:\n",
    "            for data in test_set[group]:\n",
    "                vote, _ = k_nearest_neighbors(train_set, data, k=k)\n",
    "                if group == vote:\n",
    "                    correct += 1\n",
    "                total += 1\n",
    "        accuracies.append(correct / total)\n",
    "    return sum(accuracies) / len(accuracies)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing the Model\n",
    "This main script loads the dataset, preprocesses it, and evaluates the KNN model. It handles missing values, removes outliers, and normalizes the features. The data is shuffled and split into training and testing sets. Cross-validation is used to find the best k, and the model's accuracy on the test set is then calculated and displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy: 99.21%\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    df = pd.read_csv(r\"C:\\Users\\arsha\\OneDrive - Manipal Academy of Higher Education\\Desktop\\Cryptonite\\Sample_Datasets\\knn_dataset.csv\")\n",
    "    df.replace('?', np.nan, inplace=True)\n",
    "    df.dropna(inplace=True)\n",
    "    class_column = df.columns[-1]\n",
    "    full_data = df.astype(float).values.tolist()\n",
    "    full_data = remove_outliers(full_data)\n",
    "    full_data = normalize_dataset(full_data)\n",
    "    random.shuffle(full_data)\n",
    "    test_size = 0.2\n",
    "    train_set = {cls: [] for cls in set(row[-1] for row in full_data)}\n",
    "    test_set = {cls: [] for cls in set(row[-1] for row in full_data)}\n",
    "    training_data = full_data[:-int(test_size * len(full_data))]\n",
    "    testing_data = full_data[-int(test_size * len(full_data)):]\n",
    "    for row in training_data:\n",
    "        train_set[row[-1]].append(row[:-1])\n",
    "    for row in testing_data:\n",
    "        test_set[row[-1]].append(row[:-1])\n",
    "    best_k = 1\n",
    "    best_accuracy = 0\n",
    "    for k in range(1, 11):  \n",
    "        accuracy = cross_validate(full_data, k=k, folds=5)\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_k = k\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for group in test_set:\n",
    "        for data in test_set[group]:\n",
    "            vote, confidence = k_nearest_neighbors(train_set, data, k=best_k)\n",
    "            if group == vote:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "    print(f\"Test Set Accuracy: {(correct / total) * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
