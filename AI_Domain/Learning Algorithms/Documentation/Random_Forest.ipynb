{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "## Introduction\n",
    "Random forests are essentially a collection of decision trees that are each fit on a subsample of the data. While an individual tree is typically noisey and subject to high variance, random forests average many different trees to reduce the variability and leave us with a powerful classifier. Random forests are also non-parametric and require little to no parameter tuning. They differ from many common machine learning models used today that are typically optimized using gradient descent. Models like linear regression, support vector machines, neural networks, etc. require a lot of matrix based operations, while on the other hand tree based models like random forest are constructed with just basic arithmetic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation of Algorithm in Python\n",
    "### Import Required Libraries\n",
    "Import the necessary libraries: numpy for numerical operations, pandas for data manipulation, sklearn for the diabetes dataset, and matplotlib for plotting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the Data\n",
    "We list out the features by taking all columns except the last one (df.columns[:-1]), which is presumed to be the target variable. We determine the number of samples in our training set (90% of the total) and store it in nb_train.\n",
    "Using sample, we shuffle the dataset, ensuring that rows are in random order to prevent any ordering biases.\n",
    "Finally, we split our data into training and testing sets. X_train and y_train contain the features and labels for training, while X_test and y_test hold the data for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\arsha\\OneDrive - Manipal Academy of Higher Education\\Desktop\\Cryptonite\\Sample_Datasets\\random_forest_dataset.csv\")\n",
    "features = df.columns[:-1].tolist()\n",
    "nb_train = int(np.floor(0.9 * len(df)))\n",
    "df = df.sample(frac=1, random_state=207)\n",
    "X_train = df[features][:nb_train]\n",
    "y_train = df.iloc[:, -1][:nb_train].values\n",
    "X_test = df[features][nb_train:]\n",
    "y_test = df.iloc[:, -1][nb_train:].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Entropy\n",
    "The entropy function calculates the level of impurity or randomness in a dataset. Entropy helps to measure the uncertainty of data labels, which is essential for assessing the quality of splits in a decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(p):\n",
    "    if p == 0:\n",
    "        return 0\n",
    "    elif p == 1:\n",
    "        return 0\n",
    "    else:\n",
    "        return - (p * np.log2(p) + (1 - p) * np.log2(1-p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Information Gain\n",
    "Parent is the combined dataset from left_child and right_child.\n",
    "We calculate the proportion of positive samples in the parent, left child, and right child nodes as p_parent, p_left, and p_right, respectively.\n",
    "We compute the entropy for each of these nodes (IG_p, IG_l, and IG_r).\n",
    "Finally, we calculate the information gain by subtracting the weighted entropy of the children from the parent entropy. This IG helps to find the optimal split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def information_gain(left_child, right_child):\n",
    "    parent = left_child + right_child\n",
    "    p_parent = parent.count(1) / len(parent) if len(parent) > 0 else 0\n",
    "    p_left = left_child.count(1) / len(left_child) if len(left_child) > 0 else 0\n",
    "    p_right = right_child.count(1) / len(right_child) if len(right_child) > 0 else 0\n",
    "    IG_p = entropy(p_parent)\n",
    "    IG_l = entropy(p_left)\n",
    "    IG_r = entropy(p_right)\n",
    "    return IG_p - len(left_child) / len(parent) * IG_l - len(right_child) / len(parent) * IG_r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Bootstrap Samples\n",
    "The draw_bootstrap function generates bootstrapped samples by randomly selecting data points with replacement. This sampling is central to the Random Forest approach, allowing each tree to see a different data subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_bootstrap(X_train, y_train):\n",
    "    bootstrap_indices = list(np.random.choice(range(len(X_train)), len(X_train), replace = True))\n",
    "    oob_indices = [i for i in range(len(X_train)) if i not in bootstrap_indices]\n",
    "    X_bootstrap = X_train.iloc[bootstrap_indices].values\n",
    "    y_bootstrap = y_train[bootstrap_indices]\n",
    "    X_oob = X_train.iloc[oob_indices].values\n",
    "    y_oob = y_train[oob_indices]\n",
    "    return X_bootstrap, y_bootstrap, X_oob, y_oob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the Optimal Split Point\n",
    "Randomly sample up to max_features features from X_bootstrap.\n",
    "For each feature and potential split point within it, we create two child nodes (left_child and right_child).\n",
    "Continuous values are split based on whether each value is greater or less than the split point.\n",
    "We calculate information gain for each split and select the one with the highest gain, setting it as the best node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_split_point(X_bootstrap, y_bootstrap, max_features):\n",
    "    feature_ls = list()\n",
    "    num_features = len(X_bootstrap[0])\n",
    "    while len(feature_ls) <= max_features:\n",
    "        feature_idx = random.sample(range(num_features), 1)\n",
    "        if feature_idx not in feature_ls:\n",
    "            feature_ls.extend(feature_idx)\n",
    "    best_info_gain = -999\n",
    "    node = None\n",
    "    for feature_idx in feature_ls:\n",
    "        for split_point in X_bootstrap[:,feature_idx]:\n",
    "            left_child = {'X_bootstrap': [], 'y_bootstrap': []}\n",
    "            right_child = {'X_bootstrap': [], 'y_bootstrap': []}\n",
    "            if type(split_point) in [int, float]:\n",
    "                for i, value in enumerate(X_bootstrap[:,feature_idx]):\n",
    "                    if value <= split_point:\n",
    "                        left_child['X_bootstrap'].append(X_bootstrap[i])\n",
    "                        left_child['y_bootstrap'].append(y_bootstrap[i])\n",
    "                    else:\n",
    "                        right_child['X_bootstrap'].append(X_bootstrap[i])\n",
    "                        right_child['y_bootstrap'].append(y_bootstrap[i])\n",
    "            else:\n",
    "                for i, value in enumerate(X_bootstrap[:,feature_idx]):\n",
    "                    if value == split_point:\n",
    "                        left_child['X_bootstrap'].append(X_bootstrap[i])\n",
    "                        left_child['y_bootstrap'].append(y_bootstrap[i])\n",
    "                    else:\n",
    "                        right_child['X_bootstrap'].append(X_bootstrap[i])\n",
    "                        right_child['y_bootstrap'].append(y_bootstrap[i])\n",
    "            \n",
    "            split_info_gain = information_gain(left_child['y_bootstrap'], right_child['y_bootstrap'])\n",
    "            if split_info_gain > best_info_gain:\n",
    "                best_info_gain = split_info_gain\n",
    "                left_child['X_bootstrap'] = np.array(left_child['X_bootstrap'])\n",
    "                right_child['X_bootstrap'] = np.array(right_child['X_bootstrap'])\n",
    "                node = {'information_gain': split_info_gain, \n",
    "                        'left_child': left_child, \n",
    "                        'right_child': right_child, \n",
    "                        'split_point': split_point,\n",
    "                        'feature_idx': feature_idx}\n",
    "    return node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Terminal Nodes\n",
    "The terminal_node function assigns a final class to a node once it reaches a stopping criterion. This final class becomes the prediction of the tree for that branch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def terminal_node(node):\n",
    "    y_bootstrap = node['y_bootstrap']\n",
    "    pred = max(y_bootstrap, key=y_bootstrap.count)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting Nodes Recursively\n",
    "The split_node function is a recursive function that splits a node into left and right children based on the best split found. It stops splitting when certain conditions are met, such as reaching the maximum depth, a minimum number of samples, or when a terminal node is reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_node(node, max_features, min_samples_split, max_depth, depth):\n",
    "    left_child = node['left_child']\n",
    "    right_child = node['right_child']    \n",
    "    del(node['left_child'])\n",
    "    del(node['right_child'])\n",
    "    if len(left_child['y_bootstrap']) == 0 or len(right_child['y_bootstrap']) == 0:\n",
    "        empty_child = {'y_bootstrap': left_child['y_bootstrap'] + right_child['y_bootstrap']}\n",
    "        node['left_split'] = terminal_node(empty_child)\n",
    "        node['right_split'] = terminal_node(empty_child)\n",
    "        return\n",
    "    if depth >= max_depth:\n",
    "        node['left_split'] = terminal_node(left_child)\n",
    "        node['right_split'] = terminal_node(right_child)\n",
    "        return node\n",
    "    if len(left_child['X_bootstrap']) <= min_samples_split:\n",
    "        node['left_split'] = terminal_node(left_child)\n",
    "    else:\n",
    "        node['left_split'] = find_split_point(left_child['X_bootstrap'], left_child['y_bootstrap'], max_features)\n",
    "        split_node(node['left_split'], max_features, min_samples_split, max_depth, depth + 1)\n",
    "    if len(right_child['X_bootstrap']) <= min_samples_split:\n",
    "        node['right_split'] = terminal_node(right_child)\n",
    "    else:\n",
    "        node['right_split'] = find_split_point(right_child['X_bootstrap'], right_child['y_bootstrap'], max_features)\n",
    "        split_node(node['right_split'], max_features, min_samples_split, max_depth, depth + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Decision Tree\n",
    "The root node is determined by finding the best split point on the entire bootstrapped dataset.\n",
    "The split_node function is called to split this root node and continue branching recursively until a stopping condition is met. The completed tree is returned as root_node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tree(X_bootstrap, y_bootstrap, max_depth, min_samples_split, max_features):\n",
    "    root_node = find_split_point(X_bootstrap, y_bootstrap, max_features)\n",
    "    split_node(root_node, max_features, min_samples_split, max_depth, 1)\n",
    "    return root_node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Predictions with a Decision Tree\n",
    "We check if the value of the test sample at feature_idx is less than or equal to the split point in the tree.\n",
    "Depending on the split, we either move to the left or right child node.\n",
    "If a child node is a terminal node, we return the predicted value; otherwise, we continue traversing until reaching a terminal node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tree(tree, X_test):\n",
    "    feature_idx = tree['feature_idx']\n",
    "    if X_test[feature_idx] <= tree['split_point']:\n",
    "        if type(tree['left_split']) == dict:\n",
    "            return predict_tree(tree['left_split'], X_test)\n",
    "        else:\n",
    "            value = tree['left_split']\n",
    "            return value\n",
    "    else:\n",
    "        if type(tree['right_split']) == dict:\n",
    "            return predict_tree(tree['right_split'], X_test)\n",
    "        else:\n",
    "            return tree['right_split']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Out-Of-Bag (OOB) Score Calculation\n",
    "The oob_score function calculates the error rate for Out-Of-Bag samples, giving an unbiased estimate of model accuracy on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oob_score(tree, X_test, y_test):\n",
    "    mis_label = 0\n",
    "    for i in range(len(X_test)):\n",
    "        pred = predict_tree(tree, X_test[i])\n",
    "        if pred != y_test[i]:\n",
    "            mis_label += 1\n",
    "    return mis_label / len(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Random Forest\n",
    "The random_forest function builds an ensemble of decision trees by bootstrapping data and calculating the OOB error for each tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest(X_train, y_train, n_estimators, max_features, max_depth, min_samples_split):\n",
    "    tree_ls = list()\n",
    "    oob_ls = list()\n",
    "    for i in range(n_estimators):\n",
    "        X_bootstrap, y_bootstrap, X_oob, y_oob = draw_bootstrap(X_train, y_train)\n",
    "        tree = build_tree(X_bootstrap, y_bootstrap, max_features, max_depth, min_samples_split)\n",
    "        tree_ls.append(tree)\n",
    "        oob_error = oob_score(tree, X_oob, y_oob)\n",
    "        oob_ls.append(oob_error)\n",
    "    print(\"OOB estimate: {:.2f}\".format(np.mean(oob_ls)))\n",
    "    return tree_ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Predictions with the Random Forest\n",
    "For each sample in X_test, predictions are generated by each tree in tree_ls and stored in ensemble_preds.\n",
    "The majority vote is applied to the predictions to determine the final label for each sample (final_pred).\n",
    "The final predictions are returned as a NumPy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rf(tree_ls, X_test):\n",
    "    pred_ls = list()\n",
    "    for i in range(len(X_test)):\n",
    "        ensemble_preds = [predict_tree(tree, X_test.values[i]) for tree in tree_ls]\n",
    "        final_pred = max(ensemble_preds, key=ensemble_preds.count)\n",
    "        pred_ls.append(final_pred)\n",
    "    return np.array(pred_ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Evaluating the Random Forest Model\n",
    "We define hyperparameters for the Random Forest: number of trees (n_estimators), maximum features per split (max_features), tree depth (max_depth), and minimum samples per split (min_samples_split).\n",
    "Using random_forest, we train the model on X_train and y_train.\n",
    "The trained model makes predictions on X_test using predict_rf.\n",
    "Finally, we compute and print the accuracy by comparing predictions (preds) with true labels (y_test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOB estimate: 0.50\n",
      "Testing accuracy: 0.56\n"
     ]
    }
   ],
   "source": [
    "n_estimators = 100\n",
    "max_features = 3\n",
    "max_depth = 10\n",
    "min_samples_split = 2\n",
    "model = random_forest(X_train, y_train, n_estimators=100, max_features=3, max_depth=10, min_samples_split=2)\n",
    "preds = predict_rf(model, X_test)\n",
    "acc = sum(preds == y_test) / len(y_test)\n",
    "print(\"Testing accuracy: {}\".format(np.round(acc,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theory for Random Forest\n",
    "### Entropy\n",
    "Entropy is a measurement of impurity (uncertainty) that uses the following formula:\n",
    "\n",
    "$$H(X) = -\\sum_{j} p_{j} \\log_{2} p_{j}$$\n",
    "\n",
    "where $p_{j}$ is the probability of class $j$. In the case of binary classification, entropy takes on the form:\n",
    "\n",
    "$$H(X) = -p \\log_{2} p - (1-p) \\log_{2} (1-p)$$\n",
    "\n",
    "where $p$ denotes $P(X=1)$ (the probability of a positive outcome). Also, note that in the case of binary classification, we use $\\log_{2}$.  \n",
    "Entropy follows a very intuitive interpretation with the following properties:\n",
    "\n",
    "- Certainty: Entropy is minimized when all samples in a node belong to the same class such that $P(X = 1) = 1$ (in our case every passenger survives)\n",
    "  $$\n",
    "  -1 \\log_2(1) - 0 \\log_2(0) = 0\n",
    "  $$\n",
    "\n",
    "- Uncertainty: Entropy is maximized when we have a uniform class distribution such that $P(X = 1) = 0.5$ (in our case each passenger has a 50% chance of surviving)\n",
    "$$\n",
    "-0.5 \\log_2(0.5) - 0.5 \\log_2(0.5) = 0.5 + 0.5 = 1\n",
    "$$\n",
    "\n",
    "When we are looking for a training set to split, we'll want to find a set that maximizes entropy such that half the outcomes are positive and the other half negative, so we'll want to start uncertain. When we are looking for a value to split, we'll want to minimize entropy, so we'll want to end as certain as possible."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
